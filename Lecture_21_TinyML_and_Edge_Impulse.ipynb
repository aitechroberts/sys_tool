{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>TinyML & Edge Impulse</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li>Challenges of Centralized Machine Learning</li>\n",
    "<li>What is TinyML?</li>\n",
    "<li>TinyML Application Design Flow</li>\n",
    "<li>TinyML Software Suites</li>\n",
    "<li>TinyML Software Suites</li>\n",
    "    <li>What is Edge Impulse?</li>\n",
    "    <li>Case Study: Build a Machine Learning Model to Switch On/Off Devices using Voice Commands</li>      \n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Food for Thought!!</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's watch this <a href=\"https://www.youtube.com/watch?v=ySQTcCOziXs\"> video</a> here: https://www.youtube.com/watch?v=ySQTcCOziXs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Moving ML from the Cloud to the Edge</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>Most of data going to cloud is from IoT devices</li>\n",
    "    <li>Around 328.77 million terabytes of data are generated each day. <a href=\"https://explodingtopics.com/blog/data-generated-per-day\">Check this reference for more information</a></li>\n",
    "    <li>Less than half of structured data is actively used in decision making</li>\n",
    "    <li>Less than 1% of unstructured data is analyzed or used at all\n",
    "    <li>ML at the edge can help use that data locally to make decision\n",
    "    <li>ML + low-power, low-cost devices + low latency = <b>NEW OPPORTUNITIES</b>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>What is TinyML?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li><b>Tiny Machine Learning (TinyML)</b> is a fast-growing field of machine learning technologies and applications including algorithms, hardware, and software capable of performing on-device sensor data analytics at extremely low power consumption, typically in the mW range and below.</li>\n",
    "    <li>TinyML enables a variety of always-on ML use-cases on battery-operated\n",
    "devices</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>TinyML Domain Interaction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><figure><img src=\"https://www.andrew.cmu.edu/user/mfarag/static/tinyml_domains.png\"/><figcaption>TinyML lies at the Intersection</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>TinyML Application Design Flow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><figure><img src=\"https://www.andrew.cmu.edu/user/mfarag/static/tinyml_design_flow.png\"/><figcaption>TinyML App Design Flow</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>TinyML Software Suites</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "<li><b>TensorFlow Lite:</b> An open-source framework for deploying machine learning models on edge devices.</li>\n",
    "<li><b>Arm Cortex-M microcontroller-based machine learning:</b> A suite of tools and libraries for implementing machine learning algorithms on Arm Cortex-M microcontrollers.</li>\n",
    "<li><b>Zephyr Project:</b> An open-source real-time operating system for building and deploying machine learning algorithms on IoT devices.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<ul>\n",
    "<li><b>Arduino and Arduino Create:</b> An open-source platform for building and programming electronic devices, including those with machine learning capabilities. Create is an online platform for creating and deploying IoT applications, including those using TinyML algorithms.</li>\n",
    "    <li><b>Edge Impulse:</b> A platform for developing and deploying machine learning algorithms on edge devices.</li>\n",
    "<li><b>OpenMV:</b> An open-source hardware and software platform for developing vision-based applications on microcontrollers.</li>\n",
    "<li><b>PYNQ:</b> This is an open-source framework for developing ML algorithms on Xilinx Field-Programmable Gate Arrays (FPGAs).</li>\n",
    "    <li><b>MicroPython:</b> An open-source Python interpreter that can be used to develop and run TinyML algorithms on microcontrollers and other low-power devices.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this course, we will use <b>Edge Impulse</b> for the following reasons:\n",
    "- Ease of use with customized models.\n",
    "- Requires minimal setup on your machine.\n",
    "- Offers a Browser-based solution that is compatible with every operating system.\n",
    "- Supports wide number of development boards.\n",
    "- Scalable and integrates well with other tools.\n",
    "- Offers low-latency and energy efficient solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Edge Impulse Process Flow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Edge Impulse reduces the number of tasks required to design ML application to 5 UI-led tasks:\n",
    "<ul>\n",
    "    <li>Collect Data</li>\n",
    "    <li>Clean Data and Generate Features</li>\n",
    "    <li>Train Your Model and Test It</li>\n",
    "    <li>Find the Best Model for Your Target Device</li>\n",
    "    <li>Deploy the Model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Case Study: Build a Machine Learning Model to Turn On/Off Devices using Voice Commands</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>1. Create Edge Impulse Account</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we start using Edge Impulse, you need to create a community edition account using your @andrew.cmu.edu email. Use this URL to sign up: <a href=\"https://studio.edgeimpulse.com/signup\">https://studio.edgeimpulse.com/signup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>2. Create the Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The dataset that we will use contains Speech commands for more than 60,000 instances of one-second-long utterances of 30 different spoken keywords such as \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", and many more.  These commands are stored as digital audio files. The audio files are sampled at 16 kilohertz. You may refer to the paper\n",
    "<i>\"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\" by Pete Warden to know more about the dataset.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cEU1K3PmcD0",
    "outputId": "cea5edc1-08ad-4037-d8c3-64a8b826721c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U -q tensorflow tensorflow_datasets\n",
    "#!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi8wHr_TFM5w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.1 Import the necessary packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bMtxhp5e9TlR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mfarag\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy import random\n",
    "import shutil, errno\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBcsNxXHF1TO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.2 Download the speech commands dataset</h2>\n",
    "A new directory named data will be created, which will contain subdirectories corresponding to all 30 keywords. Each subdirectory will contain the corresponding audio files in WAV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUXb5p7EsPqT",
    "outputId": "678ee160-80df-4529-e782-f897ffaa10a6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "if not data_dir.exists():\n",
    "    tf.keras.utils.get_file(\n",
    "      'speech_commands.zip',\n",
    "      origin='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>We want to develop a Turn on/Off Switch that includes keyword spotting algorithm so it can detect the target keywords \"on\" and \"off\". However, creating a binary classifier may not be sufficient, as the input audio could contain human speech with other words or may have no background audio at all</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.3 Our Multi-class Classifier</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will create a multi-class classifier to detect four different class labels: \"on\", \"off\", \"others\", and \"silent\". So, we\n",
    "need to form a new dataset from the original Speech Commands dataset containing audio instances corresponding to the four target classes we are going to detect. We will create a new empty directory, mydatset, under the parent directory data and we will populate the labels accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmPhwtkTF6Hy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.4 Get different class labels</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJgHQbgzoYNF",
    "outputId": "bf6e7bd2-5816-4226-ad2f-8a142555a005",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['.DS_Store' 'backward' 'bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five'\n",
      " 'follow' 'forward' 'four' 'go' 'happy' 'house' 'learn' 'left' 'LICENSE'\n",
      " 'marvin' 'mydataset' 'nine' 'no' 'off' 'on' 'one' 'right' 'seven'\n",
      " 'sheila' 'six' 'speech_commands.zip' 'stop' 'testing_list.txt' 'three'\n",
      " 'tree' 'two' 'up' 'validation_list.txt' 'visual' 'wow' 'yes' 'zero'\n",
      " '_background_noise_']\n"
     ]
    }
   ],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str('data')))\n",
    "commands = commands[commands != 'README.md']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-hJiQTSGPvF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.5 Form the directory for string on, off, others and silent data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0_ysXkTESQkv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('data/mydataset')\n",
    "except Exception as e:\n",
    "    print(\"Error creating folder. Error details {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def copy_data(src, dst):\n",
    "    try:\n",
    "        shutil.copytree(src, dst)\n",
    "    except OSError as exc: # python >2.5\n",
    "        if exc.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "            shutil.copy(src, dst)\n",
    "        else: raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "copy_data('data/on','data/mydataset/on')\n",
    "copy_data('data/off','data/mydataset/off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0OeJXd-PooK-",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You may use the following code instead of calling the copy_data() function above if you are not using windows\n",
    "#!cp -r 'data/on' 'data/mydataset'\n",
    "#!cp -r 'data/off' 'data/mydataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "I7HoD12qSpH_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "os.mkdir('data/mydataset/silent')\n",
    "os.mkdir('data/mydataset/others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>2.5.1 Populate \"Others\" label data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FKHF_MymS1PP",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "other_labels = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\",\"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\"]\n",
    "sample_per_label = 150\n",
    "\n",
    "\n",
    "for label in other_labels:\n",
    "    path = 'data/'+label\n",
    "    f = os.listdir(path)\n",
    "    num_files = len(f)\n",
    "    file_indx = np.arange(num_files)\n",
    "    random.shuffle(file_indx)\n",
    "    for i in range(sample_per_label):\n",
    "        index = file_indx[i]\n",
    "        file_name = f[index]\n",
    "        source = path + '/' + file_name\n",
    "        destination = 'data/mydataset/others/' + file_name\n",
    "        # copy only files\n",
    "        if os.path.isfile(source):\n",
    "            shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWm4OJ8MGb4r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>2.5.2 Create \"Silent\" label data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EZSHQbIWqwWl",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 16000\n",
    "num_files = 2400\n",
    "\n",
    "for i in range(num_files):\n",
    "    sample = np.zeros(fs)\n",
    "    filename = str(i*100)+'silent.wav'\n",
    "    sample = sample + 0.01*i*random.randn(fs)\n",
    "    scipy.io.wavfile.write('data/mydataset/silent/'+filename, fs, sample.astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>2.6 Upload the Data to Edge Impulse</h2>\n",
    "Use the data inside <b>mydataset</b> folder. Follow the Demo Carefully and watch the Lecture recording if needed. It's OK if some of the silent data were not uploaded due to identical hash values. You may also try to create the silent files yourself and upload them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>3. Add a Processing Block to Generate Features for ML Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will use <b>Mel Frequency Cepstral Coefficient (MFCC)</b> algorithm for training our model. MFCC is an audio feature extraction algorithm commonly used in speech processing and speaker recognition applications. In MFCC, the frequency bands of the audio are mapped on the Mel scale, which closely approximates the human auditory system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MFCC is considered as a better feature extraction technique compared\n",
    "to the spectrogram-based approach in speech processing applications. The MFCC\n",
    "components are calculated through the following steps:\n",
    "1. Break the audio signals into small windows.\n",
    "2. Calculate the spectrum of the windows via Short-Time Fourier Transform.\n",
    "3. Map the spectrum power into equivalent Mel scale.\n",
    "4. Calculate the logarithm of the power components at the Mel frequencies.\n",
    "5. Calculate the discrete cosine transform of the logarithm powers.\n",
    "6. The resulting amplitudes represent the MFCC values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>4. Add a Learning Block to Train Your Model and Test It</h1>\n",
    "Keep in mind, the device we will target for this example is: <b>Arduino Nano 33 BLE Sense (Cortex-M4F 64MHz)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>5. Add a new Device as Your Smartphone using QR Code and Use Your Phone to Conduct Live Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Is this the best model for the Target Hardware?</h1>\n",
    "We will answer this question next lecture!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
