{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>PyTorch: Computation Graph and GPU </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation graph is the internal mechanism that PyTorch uses to compute gradient. The way it works is that it will record all tensor computations, and for each tensor, it will ''remember'' where the tensor is computed from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "z = x*x # conduct some tensor computation\n",
    "\n",
    "print(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `z` tensor has value `4.0` as expected. Moreover, the `z` tensor includes `grad_fn=<MulBackward0>`. This is an internal object that remembers that `z` is computed from a multiplication of `x`. Such connections betweens tensors collectively form a computation graph. \n",
    "\n",
    "When you call `z.backward()`, it will look at where `z` is computed from, which in our case is $z = x^2$. It will then apply the appropriate differentiation rule $\\frac{dz}{dx} = 2x$ to compute what is the gradient of $z$ with respect to $x$, which in our case is 2*2 = 4. The result of the gradient computation is stored in `x.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of z = x*x respect to x is : tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "\n",
    "print(\"The gradient of z = x*x respect to x is :\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Revisit: Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "class MyLinearRegressionModel(nn.Module): \n",
    "    def __init__(self,d): # d is the dimension of the input\n",
    "        super(MyLinearRegressionModel,self).__init__()   # call the init function of super class\n",
    "        # we usually create variables for all our model parameters (w and b in our case) in __init__ and give them initial values. \n",
    "        # need to create them as nn.Parameter so that the model knows it is an parameter that needs to be trained\n",
    "        self.w = nn.Parameter(torch.zeros(1,d, dtype=torch.float)) \n",
    "        self.b = nn.Parameter(torch.zeros(1,dtype=torch.float))\n",
    "    def forward(self,x):\n",
    "        # The main purpose of the forward function is to specify given input x, how the output is calculated. \n",
    "        return torch.inner(x,self.w) + self.b\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, you will find that the \"w\" and \"b\" will have `requires_grad = True`. This is because we created w and b as `nn.Parameter`, a special type of tensor for model's trainable parameters. By default, `nn.Parameter` will have `requires_grad = True` as  `nn.Parameter` is supposed to be \"trainable\", that is, we want to do gradient descent on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mymodel = MyLinearRegressionModel(1) # creating a model instance with input dimension 1\n",
    "print(mymodel.w)\n",
    "print(mymodel.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is in contrast to the tensor for the dataset. For example, recall we created the `x, y` tensor as the data set to train on. By default, such non-parameter tensors have `requires_grad = False`. The reason is we don't expect to do gradient descent on `x, y` (which are just some input and output data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad = False\n",
      "y.requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,10,.1,dtype=torch.float) \n",
    "x = x[:,None]\n",
    "y = x*3+torch.randn(x.shape)\n",
    "\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, after the forward and backward, the gradient is only computed for `mymodel.w` and `mymodel.b` which has `requires_grad = True`. The gradient is not computed for `x` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel.w.grad = tensor([[-195.5122]]), mymodel.b.grad = tensor([-29.5709])\n",
      "x.grad = None, y.grad = None\n"
     ]
    }
   ],
   "source": [
    "prediction = mymodel(x)\n",
    "loss = torch.mean((prediction - y)**2)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(f\"mymodel.w.grad = {mymodel.w.grad}, mymodel.b.grad = {mymodel.b.grad}\")\n",
    "print(f\"x.grad = {x.grad}, y.grad = {y.grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case is \"non-leaf\" tensors, which are tensors resulting from some computation from other tensors. As an example,the `prediction = mymodel(x)` tensor is from some computation from `x`, and `mymodel.w`, `mymodel.b`.  \n",
    "\n",
    "If such \"non-leaf\" tensor is computed from at least one tensor with `requires_grad = True`, then this tensor will also have `requires_grad = True`. For the `prediction` tensor, since `mymodel.w, mymodel.b` have `requires_grad = True`, so `prediction` will also have `requires_grad = True`. This is because of the nature of the back-propagation, the algorithm underlying `backward()`. When calling `backward()`, the gradient for `prediction` has to be computed first before the gradient for `mymodel.w` and `mymodel.b` can be computed, so `prediction` will also have `requires_grad = True`. That being said, after the `backward()`, the gradient for such non-leaf tensors will be discarded as they are only an intermediary result and not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.requires_grad = True, prediction.grad = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/z18mrwfd6xvc5_xsmtvk3yym0000gn/T/ipykernel_78835/1494579461.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"prediction.requires_grad = {prediction.requires_grad}, prediction.grad = {prediction.grad}\" )\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction.requires_grad = {prediction.requires_grad}, prediction.grad = {prediction.grad}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach method\n",
    "The `detach()` method detaches a tensor from a computation graph - it will become an independent tensor with `requires_grad = False`, and are not connected to other tensors any more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(291.9410, grad_fn=<MeanBackward0>) loss.requires_grad = True\n",
      "loss_detached =  tensor(291.9410) loss_detached.requires_grad = False\n"
     ]
    }
   ],
   "source": [
    "print(\"loss = \", loss, f\"loss.requires_grad = {loss.requires_grad}\")\n",
    "loss_detached = loss.detach()\n",
    "print(\"loss_detached = \", loss_detached, f\"loss_detached.requires_grad = {loss_detached.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detaching a tensor is necessary to convert a tensor with `requres_grad = True` to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_detached.numpy() =  291.941\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/coolq/Library/CloudStorage/Box-Box/Teaching/Tool Chain/Toolchain 2023 Fall/notebooks/Lecture_18_pytorch_computation_graph.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/coolq/Library/CloudStorage/Box-Box/Teaching/Tool%20Chain/Toolchain%202023%20Fall/notebooks/Lecture_18_pytorch_computation_graph.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss_detached.numpy() = \u001b[39m\u001b[39m\"\u001b[39m, loss_detached\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/coolq/Library/CloudStorage/Box-Box/Teaching/Tool%20Chain/Toolchain%202023%20Fall/notebooks/Lecture_18_pytorch_computation_graph.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss.numpy() = \u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39;49mnumpy())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "print(\"loss_detached.numpy() = \", loss_detached.numpy())\n",
    "\n",
    "print(\"loss.numpy() = \", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.no_grad()\n",
    "\n",
    "Sometimes, you only want to do forward without doing the backward, e.g. in validation and testing. In this case, it would be a waste of resources if torch still builds the computation graph, as the computation graph will never be used. \n",
    "\n",
    "In this case, placing your forward pass under `with torch.no_grad()` will temporarily disable the construction of computation graph and will save computation/memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.requires_grad = True prediction.grad_fn = <AddBackward0 object at 0x7faa29906190>\n",
      "loss.requires_grad = True loss.grad_fn = <MeanBackward0 object at 0x7faa2972e1c0>\n",
      "prediction.requires_grad = False prediction.grad_fn = None\n",
      "loss.requires_grad = False loss.grad_fn = None\n"
     ]
    }
   ],
   "source": [
    "prediction = mymodel(x)\n",
    "loss = torch.mean((prediction - y)**2)\n",
    "print(f\"prediction.requires_grad = {prediction.requires_grad}\", f\"prediction.grad_fn = {prediction.grad_fn}\")\n",
    "print(f\"loss.requires_grad = {loss.requires_grad}\", f\"loss.grad_fn = {loss.grad_fn}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = mymodel(x)\n",
    "    loss = torch.mean((prediction - y)**2)\n",
    "    print(f\"prediction.requires_grad = {prediction.requires_grad}\", f\"prediction.grad_fn = {prediction.grad_fn}\")\n",
    "    print(f\"loss.requires_grad = {loss.requires_grad}\", f\"loss.grad_fn = {loss.grad_fn}\")\n",
    "    # if you try to do loss.backward() here, an error will occur\n",
    "    # loss.backward()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coolq/opt/anaconda3/envs/sparktest2/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/coolq/opt/anaconda3/envs/sparktest2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "print(resnet18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Unfreeze last layer\n",
    "for param in resnet18.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 7.9002814292907715\n",
      "Epoch 2/5, Loss: 7.787904262542725\n",
      "Epoch 3/5, Loss: 7.574479579925537\n",
      "Epoch 4/5, Loss: 7.270318508148193\n",
      "Epoch 5/5, Loss: 6.884873867034912\n"
     ]
    }
   ],
   "source": [
    "# Create random data\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "labels = torch.randint(0, 10, (5,))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet18.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = resnet18(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/5, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward/zero_grad/backward/step procedures in the training loop can be parallized on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading torch.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether a GPU is available on your computer, you can run `torch.cuda.is_available()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.cuda\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check how many GPUs are available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purpose, we will train a LeNet to learn how to recognize handwritten digits for the MNIST dataset. \n",
    "\n",
    "LeNet is a convolutional neural network proposed by Yann Lecun in the 80s. The code of LeNet is as below. The details of how a convolutional neural network works is beyond the scope of this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realization of LeNet \n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Max pool 2-d\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=256, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training with GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct the forward/zero_grad/backward/step procedures with GPU acceleration, you need to move the model and the data to the GPU device. \n",
    "\n",
    "For example, suppose the neural network model is named `mynn`, then run `mynn = mynn.to(device = device)` to move the model to a given device. For GPU device, set `device = torch.device('cuda:0')`, where `cuda:0` means the default GPU. \n",
    "\n",
    "Similarly, upon loading `x_batch, y_batch` from the dataloader, run\n",
    "```\n",
    "x_batch = x_batch.to(device)\n",
    "y_batch = y_batch.to(device)\n",
    "```\n",
    "to move `x_batch, y_batch` to GPU. \n",
    "\n",
    "Once the model and the data are on the GPU, all the subsequent forward/zero_grad/backward/step procedures will automatically be implemented in a parallized manner on GPU.\n",
    "\n",
    "You may not be able to run the following code on your local computer because you may not have GPU on your computer. You can run the code on CoLab with GPU enabled, and for convenience, we have created a seperate notebook `lecture_20_gpu_demo.ipynb` for you to upload to CoLab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, device = cpu, per_batch_time = 0.0032194700876871747, train_loss = 0.8680732250213623\n",
      "Epoch = 1, device = cpu, per_batch_time = 0.003127245203653971, train_loss = 0.4853483736515045\n",
      "Epoch = 2, device = cpu, per_batch_time = 0.003118114344278971, train_loss = 0.4081261157989502\n",
      "Epoch = 3, device = cpu, per_batch_time = 0.003064979298909505, train_loss = 0.36758360266685486\n",
      "Epoch = 4, device = cpu, per_batch_time = 0.0033786468505859375, train_loss = 0.34055203199386597\n"
     ]
    }
   ],
   "source": [
    "import torch,torchvision\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Choose between CPU or GPU (cuda:0)\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0')\n",
    "\n",
    "# move model to device\n",
    "mynn = LeNet()\n",
    "mynn = mynn.to(device = device)\n",
    "\n",
    "# get dataset \n",
    "\n",
    "\n",
    "# Three hyper parameters for training\n",
    "lr = .04\n",
    "batch_size = 32\n",
    "N_epochs = 5\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "mydataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(mydataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(mynn.parameters(), lr = lr) # this line creates a optimizer, and we tell optimizer we are optimizing the parameters in mymodel\n",
    "\n",
    "losses = [] # training losses of each epoch\n",
    "num_batches = len(train_dataloader)\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    batch_loss = []\n",
    "    per_batch_time = 0.0\n",
    "    for batch_id, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "        start_time = time.time()\n",
    "        # data to device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # pass input data to get the prediction outputs by the current model\n",
    "        prediction = mynn(x_batch)\n",
    "\n",
    "        # compare prediction and the actual output label and compute the loss\n",
    "        loss = nn.functional.cross_entropy(prediction,y_batch)\n",
    "\n",
    "        # compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        per_batch_time += (end_time - start_time)\n",
    "\n",
    "        # add this loss to batch_loss for later computation\n",
    "        batch_loss.append(loss.detach().numpy())\n",
    "    \n",
    "    losses.append(np.mean(np.array(batch_loss)))\n",
    "    per_batch_time = per_batch_time/num_batches\n",
    "    print(f\"Epoch = {epoch}, device = {device}, per_batch_time = {per_batch_time}, train_loss = {losses[-1]}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8276d678390595138ef1424ae828a970263a6f746980760e1cfd17345ae3e994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
